package sober.hadoop.c45;

import java.io.*;
import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
//import org.apache.hadoop.mapred.FileInputFormat;
//import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.ArrayWritable;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class MRC45 {
	
	//一共三个全局变量
    public static CurrentSplitNode current_split = new CurrentSplitNode();  //当前待分裂的树节点
    
    public static List <CurrentSplitNode> splitted_node = new ArrayList<CurrentSplitNode>();  //split类型的list，存储已经分裂过的树节点
    
    public static int current_node_index=0;  //当前节点的索引
    
    public static void main(String[] args) throws Exception {
    	
      int split_index = 0; //为当前节点挑选的最佳分裂属性的索引
      
      double entropy = 0;
  	  double info_Gain_Ratio = 0;
  	  double best_Gain_Ratio = 0;
  	  
  	  String ClassLabel = null;
  	  int total_attributes=4; //手动指定属性的个数
  	  
  	  splitted_node.add(current_split);//初始化当前待分裂节点
  	  int num_nodes = splitted_node.size();
  	  InfoGainRatio gainObj;
  	  CurrentSplitNode newnode;
  	  
  	  while(num_nodes > current_node_index){
  		
  		//splitted_node是一个列表，存储分裂过的节点，每次取最上面的一个节点进行分裂是为了模拟队列这个数据结构
  		//通过模拟队列实现了广度优先的决策树构建
  		//有点儿屌
  		
  		current_split = (CurrentSplitNode) splitted_node.get(current_node_index);//每一次循环都会更新current_split这个变量
  		gainObj=new InfoGainRatio();
  		
  		Configuration conf = new Configuration();
  		Job job = new Job(conf, "C4.5");
  		
  		job.setJarByClass(sober.hadoop.c45.MRC45.class);
  		
  		job.setMapperClass(Map.class);
		job.setReducerClass(Reduce.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.setInputPaths(job, "../../user/hadoop/playtennis.txt");
	    FileOutputFormat.setOutputPath(job, new Path("../../user/hadoop/c45/output"+current_node_index));
	    
	    job.waitForCompletion(true);
	    
	    
  		
  		  
  	  }
    	
    	
    }

}
